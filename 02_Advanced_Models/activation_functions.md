# Activation Functions in Neural Networks

このファイルでは、PyTorchで一般的に使用される活性化関数を定義し、それぞれの特徴や使用場面について説明します。活性化関数は、ニューラルネットワークが学習するための非線形性をもたらし、モデルが複雑な関数を近似できるようにする重要な要素です。

## 1. ReLU（Rectified Linear Unit）
- **式**:
  ```
  f(x) = max(0, x)
  ```
 
- **特徴**: 入力が正の場合はそのまま出力し、負の場合は0に変換する非線形関数です。計算が高速で、勾配消失問題が起きにくいため、ディープラーニングの多層ニューラルネットワークで広く使用されています。
- **使用場面**: 畳み込みニューラルネットワーク（CNN）や多層パーセプトロン（MLP）など、さまざまなモデルで一般的に使用されます。

## 2. Leaky ReLU（Leaky Rectified Linear Unit）
- **式**: 
```
  f(x) = x  (if x > 0), 
  f(x) = αx  (if x ≤ 0), 例: α=0.1
```
- **特徴**: ReLUの変種で、負の入力に対して小さな傾きを持たせています。これにより、負の入力に対しても微小な勾配を保ち、ニューロンが完全に「死んでしまう」問題（Dead Neuron）を回避します。
- **使用場面**: CNNやMLPでReLUと同様に使われますが、特に勾配消失問題を防ぐためにLeaky ReLUが使われる場合があります。

## 3. Sigmoid
- **式**: 
```
    f(x) = 1 / (1 + exp(-x))
```
- **特徴**: 出力が0から1の間に収まるS字型の関数で、確率を表現するのに適しています。しかし、勾配消失問題が発生しやすいため、深層ネットワークにはあまり使用されません。
- **使用場面**: 出力を確率として解釈したい2値分類タスクの出力層で使用されることが一般的です。

## 4. Tanh（Hyperbolic Tangent）
- **式**: 
```
f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
```
- **特徴**: 出力が-1から1の間に収まり、ゼロ中心であるため、Sigmoidよりも勾配消失の問題が軽減されます。しかし、深い層になると依然として勾配が小さくなる可能性があります。
- **使用場面**: RNNや一部のMLPで使用されることがあります。

## 5. ELU（Exponential Linear Unit）
- **式**: 
  ```
  f(x) = x  (if x > 0),
  f(x) = α * (exp(x) - 1) (if x ≤ 0)
  ```
- **特徴**: ReLUの変種で、負の入力に対して指数関数的な変化を与えます。これにより、勾配がゼロになりすぎることを防ぎ、学習の安定性が向上します。
- **使用場面**: ReLUの代替として、特に深層ネットワークでの学習安定性を向上させたい場合に使用されます。
