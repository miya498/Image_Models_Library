# Convolutional Neural Networks (CNN) Models

このプロジェクトでは、LeNet、AlexNet、VGGNet、ResNetといった代表的なCNNアーキテクチャを実装し、CIFAR-10データセットに適用しました。それぞれのモデルについて、構造や特徴、実装の詳細を解説します。

## 1. LeNet

**概要**:
LeNetは、1990年代に手書き文字認識のために開発された初期のCNNモデルで、シンプルな構造を持ちます。小規模なデータセットでの実験や、CNNの基礎理解に適しています。

**構造**:
- 入力サイズ: 32×32（CIFAR-10のサイズに合わせています）
- 層の構成:
  - 畳み込み層（6チャンネル、カーネル5×5、ストライド1） + 平均プーリング（2×2）
  - 畳み込み層（16チャンネル、カーネル5×5、ストライド1） + 平均プーリング（2×2）
  - 全結合層（120ユニット）
  - 全結合層（84ユニット）
  - 出力層（10クラス分類）

**特徴**:
- モデルが浅いため、学習速度が速く、メモリ使用量が少ない。
- 小規模データセットや軽量なタスクに適しています。

---

## 2. AlexNet

**概要**:
AlexNetは、2012年のILSVRC（ImageNet Large Scale Visual Recognition Challenge）で高精度を実現したモデルで、深層学習の重要性を示しました。大きなカーネルサイズやプーリング層を活用して、より複雑な特徴を抽出します。

**構造**:
- 入力サイズ: 227×227（リサイズしてCIFAR-10に適用）
- 層の構成:
  - 畳み込み層（64チャンネル、カーネル11×11、ストライド4、パディング2） + Max Pooling（3×3）
  - 畳み込み層（192チャンネル、カーネル5×5、ストライド1、パディング2） + Max Pooling（3×3）
  - 畳み込み層（384チャンネル、カーネル3×3、ストライド1、パディング1）
  - 畳み込み層（256チャンネル、カーネル3×3、ストライド1、パディング1）
  - 畳み込み層（256チャンネル、カーネル3×3、ストライド1、パディング1） + Max Pooling（3×3）
  - 全結合層（4096ユニット）+ ドロップアウト
  - 全結合層（4096ユニット）+ ドロップアウト
  - 出力層（10クラス分類）

**特徴**:
- ReLU活性化関数を用いて学習の収束速度を向上。
- ドロップアウト層により、過学習を防止。
- CIFAR-10の画像サイズに合わせてリサイズして実装。

---

## 3. VGGNet

**概要**:
VGGNetは、3×3の小さなカーネルを積み重ねることで、パラメータ数を抑えつつも深層のモデルを実現しました。画像認識の精度が高く、シンプルで均一な構造が特徴です。

**構造（VGG-11）**:
- 入力サイズ: 224×224（リサイズしてCIFAR-10に適用）
- 層の構成:
  - 畳み込み層（64チャンネル、カーネル3×3、パディング1） + Max Pooling（2×2）
  - 畳み込み層（128チャンネル、カーネル3×3、パディング1） + Max Pooling（2×2）
  - 畳み込み層（256チャンネル、カーネル3×3、パディング1） * 2 + Max Pooling（2×2）
  - 畳み込み層（512チャンネル、カーネル3×3、パディング1） * 2 + Max Pooling（2×2）
  - 畳み込み層（512チャンネル、カーネル3×3、パディング1） * 2 + Max Pooling（2×2）
  - 全結合層（4096ユニット）+ ドロップアウト
  - 全結合層（4096ユニット）+ ドロップアウト
  - 出力層（10クラス分類）

**特徴**:
- 小さなカーネルサイズ（3×3）で多くの層を積み重ねることで、深いネットワーク構造を実現。
- CIFAR-10用にサイズをリサイズし、過学習防止のためドロップアウトを活用。

---

## 4. ResNet

**概要**:
ResNetは、残差接続（skip connection）を用いることで、深い層でも勾配消失問題を防ぎ、学習が安定するように設計されています。非常に深いネットワークにおいても効果的です。

**構造（ResNet-18）**:
- 入力サイズ: 32×32
- 層の構成（ResidualBlock構造）:
  - 畳み込み層（64チャンネル、カーネル3×3、ストライド1、パディング1） + Batch Normalization + ReLU
  - 残差ブロック x 2（64チャンネル、ストライド1）
  - 残差ブロック x 2（128チャンネル、ストライド2）
  - 残差ブロック x 2（256チャンネル、ストライド2）
  - 残差ブロック x 2（512チャンネル、ストライド2）
  - 平均プーリング（Adaptive Avg Pooling）
  - 出力層（10クラス分類）

**特徴**:
- 残差接続（skip connection）によって、勾配が層を越えて伝播しやすく、深い層でも安定して学習が可能。
- CIFAR-10の画像サイズに合わせて、構造を調整しています。

---

## 各モデルの学習と検証結果の可視化
各モデルの学習過程では、エポックごとのトレーニングと検証の損失および精度を記録し、学習曲線として可視化しています。これにより、各モデルの学習の安定性や収束状況を把握することができます。

- **損失曲線**: トレーニングと検証の損失が、エポックごとにどのように減少しているかを示します。過学習の発生や学習の収束具合を確認できます。
- **精度曲線**: トレーニングと検証の精度が、エポックごとにどのように変化しているかを示します。モデルの性能向上や安定性を確認できます。

各モデルにおいて、適切なハイパーパラメータの設定やドロップアウト、バッチ正規化、残差接続などの技術が、学習の安定性や過学習防止に効果的であることが確認できます。

---

これらのCNNアーキテクチャは、それぞれ異なる特徴や工夫を持ち、さまざまな画像分類タスクに適応可能です。本プロジェクトでは、これらのモデルをCIFAR-10データセットに適用し、性能を比較しながらCNNの理解を深めました。