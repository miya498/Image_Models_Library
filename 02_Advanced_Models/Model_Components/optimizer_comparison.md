# Optimizer Comparison

このドキュメントでは、GD（勾配降下法）、Momentum（モーメンタム法）、AdaGrad、Adamといった代表的な最適化手法について説明します。各手法の特徴や更新ルールについてまとめています。

## 1. GD（勾配降下法, Gradient Descent）
- **概要**: 勾配降下法は、目的関数の勾配（傾き）を計算し、その方向に沿ってパラメータを調整するシンプルな最適化手法です。最も基本的な手法で、関数の極小値を見つけることを目的としています。
- **更新ルール**: パラメータ θ は、目的関数 \( f(θ) \) の勾配に基づき、学習率 α に従って更新されます。
  ```
  θ = θ - α ∇f(θ)
  ```
- **長所**: 実装が簡単で、計算量も少なく済みます。
- **短所**: 局所的な最適解に陥りやすく、また平坦な領域や谷のような形状の関数では収束に時間がかかることがあります。

## 2. Momentum（モーメンタム法）
- **概要**: モーメンタム法は、GDに「慣性」を加えることで、勾配の方向がスムーズになり、最適解により早く到達できるようにした手法です。前のステップの勾配の更新を加速することで、収束速度を向上させます。
- **更新ルール**:
  ```
  v_t = β v_(t-1) + α ∇f(θ_(t-1))
  θ = θ - v_t
  ```
  - ここで、 β はモーメンタム係数（典型的な値は 0.9）です。
- **長所**: 平坦な領域や谷状の関数でも高速に収束し、勾配の変動が少なくなります。
- **短所**: 勾配が急激に変わる場合や複雑な最適化問題では、収束が不安定になることがあります。

## 3. AdaGrad
- **概要**: AdaGradは、パラメータごとに異なる学習率を適用する手法です。頻繁に更新されるパラメータの学習率を自動的に減少させ、まれな更新が必要なパラメータに大きな学習率を適用します。
- **更新ルール**:
  ```
  G_t = G_(t-1) + (∇f(θ_(t-1)))^2
  θ = θ - α / √(G_t + ε) * ∇f(θ)
  ```
  - ここで、 ε はゼロ割りを避けるための小さな定数です。
- **長所**: スパースなデータ（まれに現れる情報）に対して効果的で、異なるパラメータに応じた学習率調整が行われます。
- **短所**: 学習率が徐々に減少するため、学習が進むにつれて更新が小さくなりすぎ、収束が遅くなることがあります。

## 4. Adam
- **概要**: Adamは、モーメンタム法とAdaGradを組み合わせた手法で、それぞれの長所を取り入れた最適化手法です。勾配の移動平均と、パラメータごとの学習率調整を組み合わせることで、収束の速さと安定性を両立しています。
- **更新ルール**:
  ```
  m_t = β1 m_(t-1) + (1 - β1) ∇f(θ_(t-1))
  v_t = β2 v_(t-1) + (1 - β2)(∇f(θ_(t-1)))^2
  m̂_t = m_t / (1 - β1^t)
  v̂_t = v_t / (1 - β2^t)
  θ = θ - α / √(v̂_t + ε) * m̂_t
  ```
  - ここで、β1とβ2はモーメンタム係数（一般的に、β1=0.9, β2=0.999）、εは安定化のための定数です。
- **長所**: 収束が高速で、データ分布が不均一でも安定した学習が可能です。
- **短所**: 非凸関数では、局所的な最適解に陥ることがあります。また、ハイパーパラメータの調整が必要な場合もあります。

---

## まとめ

| 最適化手法 | 長所 | 短所 |
| ---------- | ---- | ---- |
| GD         | シンプルで計算量が少ない | 局所解に陥りやすく、収束が遅い場合あり |
| Momentum   | 収束が速く、変動が少ない | 勾配が急変する場合は不安定になることも |
| AdaGrad    | スパースデータに強く、学習率調整が自動 | 学習率が小さくなりすぎる傾向がある |
| Adam       | 収束が速く安定している | 局所解に陥る場合があり、ハイパーパラメータ調整が必要 |

これらの最適化手法は、目的に応じて使い分けることで、学習速度や収束性を改善することができます。
