# Regularization and Batch Normalization in MLP for MNIST Classification

このプロジェクトでは、正則化（ドロップアウト）とBatch Normalizationを多層パーセプトロン（MLP）モデルに適用し、MNISTデータセットの分類性能を評価します。これらの手法を使うことで、モデルの学習の安定性や精度の向上が期待できます。

## 正則化（ドロップアウト）
### 1. 正則化の目的
正則化（ドロップアウト）は、学習中にモデルが特定の特徴に過剰に適合すること（過学習）を防ぐために使用されます。ドロップアウトは、各学習ステップで一部のニューロンを無効にすることで、ネットワークの汎化性能を向上させます。

### 2. ドロップアウトの実装方法
- ドロップアウトレイヤーは、`torch.nn.Dropout`を使用して簡単に実装できます。
- ドロップアウトの割合（例: 50%）を指定し、訓練中にその割合でニューロンをランダムに無効にします。

### 3. ドロップアウトの効果
- 各エポックでネットワークがランダムに構成されるため、モデルが特定のニューロンに依存するのを防ぎ、汎化性能を高めます。
- トレーニングデータに対して損失がやや大きくなることがありますが、テストデータに対する精度が向上することが期待されます。

### ドロップアウトあり・なしの実験結果
- **トレーニング損失**: ドロップアウトなしのモデルは、トレーニングデータに対して低い損失を達成しますが、テストデータでは過学習の影響が出やすいです。
- **テスト精度**: ドロップアウトありのモデルは、過学習が抑制され、より高いテスト精度が得られる傾向があります。

---

## Batch Normalization
### 1. Batch Normalizationの目的
Batch Normalizationは、各バッチでの出力を正規化し、学習中の勾配の発散や消失を抑えることで、学習の安定性や収束速度を向上させます。また、より大きな学習率での学習が可能になり、トレーニングの高速化にも貢献します。

### 2. Batch Normalizationの実装方法
- Batch Normalizationは、`torch.nn.BatchNorm1d`を用いて簡単に実装できます。
- 各隠れ層の後にBatch Normalizationレイヤーを挿入し、出力を正規化します。

### 3. Batch Normalizationの効果
- 正規化により、各層の出力が一定の範囲内に収まり、学習の安定性が増します。
- 大きな学習率でも収束が早く、過学習を抑える効果も期待できます。

### Batch Normalizationあり・なしの実験結果
- **トレーニング損失**: Batch Normalizationありのモデルは、早期に損失が減少し、安定して収束します。
- **テスト精度**: Batch Normalizationありのモデルは、テストデータに対しても高い精度を達成しやすく、過学習が抑制される効果が見られます。

---

## 実装例とグラフ
正則化とBatch Normalizationを組み込んだモデルを構築し、以下の項目をグラフで比較します。
- トレーニング損失：ドロップアウトやBatch Normalizationの有無でエポックごとの損失の変動を比較します。
- テスト精度：テストデータに対する精度の推移をグラフに表示し、汎化性能の改善を確認します。

### コード実行方法
1. `MLP_WithDropout`（ドロップアウトあり）、`MLP_WithBatchNorm`（Batch Normalizationあり）などのモデルを定義します。
2. `train_model`関数を使ってモデルをトレーニングし、損失と精度を記録します。
3. `matplotlib`を使用して、エポックごとのトレーニング損失とテスト精度をプロットし、モデルの学習効果を確認します。

### 結論
正則化とBatch Normalizationを適用することで、MNISTデータセットにおけるMLPモデルの学習が安定し、テストデータに対する精度も向上しました。これらの技術を活用することで、汎化性能を高め、より良い分類結果を得ることが可能です。